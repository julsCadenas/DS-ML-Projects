{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8861846a",
      "metadata": {
        "id": "8861846a"
      },
      "source": [
        "# **Sentiment Analysis on Tweets**\n",
        "Sentiment analysis, also known as opinion mining, is the process of identifying and categorizing emotions expressed in text dataâ€”typically as positive, negative, or neutral. It helps organizations and individuals understand the sentiment behind user-generated content, such as product reviews, social media posts, or customer feedback.\n",
        "\n",
        "In the context of social media, sentiment analysis is particularly valuable due to the vast amount of real-time user opinions shared daily. Twitter, with its concise and public messages, provides an ideal dataset for analyzing public sentiment around topics, events, brands, or products.\n",
        "\n",
        "By leveraging natural language processing (NLP) techniques and machine learning models, sentiment analysis can extract insights from tweets to support business decisions, brand monitoring, political analysis, and crisis management.\n",
        "\n",
        "This project focuses on building a sentiment classifier using a dataset of tweets. The model aims to classify each tweet as positive or negative, helping reveal how people feel about certain topics at scale.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Dataset:** [Kaggle Sentiment140](https://www.kaggle.com/datasets/kazanova/sentiment140/data)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6cfb6e2",
      "metadata": {
        "id": "e6cfb6e2"
      },
      "source": [
        "## **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15b62c3e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15b62c3e",
        "outputId": "32208a2c-9142-4cbf-da29-ba4d2126856d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import emoji\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b84aa2d",
      "metadata": {
        "id": "7b84aa2d"
      },
      "source": [
        "Prepare the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d63041c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "d63041c7",
        "outputId": "fc79032f-5ad2-4b3c-ffe9-087690a9235a"
      },
      "outputs": [],
      "source": [
        "dataset_path = '../data/twt.csv'\n",
        "column_names = ['sentiment', 'id', 'date', 'flag', 'user', 'text']\n",
        "df = pd.read_csv(dataset_path, encoding='latin', delimiter=',', names=column_names)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f3f5c7f",
      "metadata": {
        "id": "8f3f5c7f"
      },
      "source": [
        "Drop unimportant columns and missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec68813b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ec68813b",
        "outputId": "1a959055-e930-4097-9b86-45963b54a9f7"
      },
      "outputs": [],
      "source": [
        "df = df.drop(['id', 'date', 'flag', 'user'], axis=1)\n",
        "df = df.dropna()\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3d47b10",
      "metadata": {
        "id": "a3d47b10"
      },
      "source": [
        "### Preprocess the data:\n",
        "- convert the emojis into text\n",
        "- lowercase everything\n",
        "- remove urls, mentions and hashtags\n",
        "- remove punctuations and special characters\n",
        "- remove stopwords\n",
        "- split into tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d29ad00d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d29ad00d",
        "outputId": "31edf4b3-893e-431b-e228-2c7eb9dcf1e1"
      },
      "outputs": [],
      "source": [
        "print(\"Sample stopwords being removed:\")\n",
        "print(list(stop_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05d9ac6b",
      "metadata": {
        "id": "05d9ac6b"
      },
      "outputs": [],
      "source": [
        "sentiment_critical = {\n",
        "    'not', 'no', 'never', 'nothing', 'nobody', 'none', 'nowhere', 'neither',\n",
        "    'very', 'really', 'quite', 'rather', 'extremely', 'incredibly', 'absolutely',\n",
        "    'but', 'however', 'although', 'though', 'yet', 'except',\n",
        "    'too', 'so', 'such', 'more', 'most', 'less', 'least',\n",
        "    'only', 'just', 'still', 'even', 'again'\n",
        "}\n",
        "\n",
        "negative_contractions = {\n",
        "    \"don't\", \"won't\", \"can't\", \"shouldn't\", \"wouldn't\", \"couldn't\",\n",
        "    \"isn't\", \"aren't\", \"wasn't\", \"weren't\", \"hasn't\", \"haven't\",\n",
        "    \"hadn't\", \"doesn't\", \"didn't\", \"won't\", \"shan't\", \"mustn't\",\n",
        "    \"mightn't\", \"needn't\"\n",
        "}\n",
        "\n",
        "sentiment_critical.update(negative_contractions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85851427",
      "metadata": {
        "id": "85851427"
      },
      "outputs": [],
      "source": [
        "def clean_twts(twt):\n",
        "    twt = twt.lower()\n",
        "    twt = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', twt)  # remove urls\n",
        "    twt = re.sub(r\"@\\w+\", '', twt)  # remove mentions\n",
        "    twt = re.sub(r\"#\", '', twt)  # remove hashtag symbol\n",
        "    twt = emoji.replace_emoji(twt, replace='')  # remove emojis\n",
        "    twt = re.sub(r\"[^a-zA-Z\\s']\", '', twt)  # remove punctuation\n",
        "    twt = re.sub(r\"\\s+\", ' ', twt).strip()  # clean whitespace\n",
        "\n",
        "    tokens = twt.split()\n",
        "    tokens = [word for word in tokens if (word not in stop_words or word in sentiment_critical) and len(word) > 1] # remove stopwords and keep sentiment-critical words\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # lemmatize\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "cleaned_twts = df['text'].apply(clean_twts)\n",
        "df['cleaned_text'] = cleaned_twts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4fb2ee0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "f4fb2ee0",
        "outputId": "59654c9a-39ff-4297-e313-f1b70d88d969"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18876e91",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18876e91",
        "outputId": "09184eb7-4301-4c54-b26e-7e5fc593fb1e"
      },
      "outputs": [],
      "source": [
        "# Check average tweet length after preprocessing\n",
        "lengths = [len(text.split()) for text in cleaned_twts]\n",
        "print(f\"Average length: {np.mean(lengths):.2f}\")\n",
        "print(f\"95th percentile: {np.percentile(lengths, 95):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81367f15",
      "metadata": {
        "id": "81367f15"
      },
      "source": [
        "Tokenize the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b5e8e8b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b5e8e8b",
        "outputId": "b2bf89aa-1b2e-4af9-c43c-f9c45298f708"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(cleaned_twts)\n",
        "sequences = tokenizer.texts_to_sequences(cleaned_twts)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=20, padding='post', truncating='post')\n",
        "\n",
        "print(\"Tokenized and padded sequences:\")\n",
        "print(padded_sequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d46cda5e",
      "metadata": {
        "id": "d46cda5e"
      },
      "source": [
        "Add into the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e0d291f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "8e0d291f",
        "outputId": "08c0a964-25ac-4000-8621-ded2f869aacc"
      },
      "outputs": [],
      "source": [
        "df['padded_text'] = list(padded_sequences)\n",
        "df['sentiment'] = df['sentiment'].map({4: 1, 0: 0})\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54b07005",
      "metadata": {
        "id": "54b07005"
      },
      "source": [
        "Split the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70cedc07",
      "metadata": {
        "id": "70cedc07"
      },
      "outputs": [],
      "source": [
        "x = padded_sequences\n",
        "y = df['sentiment']\n",
        "\n",
        "\n",
        "x_train, x_temp, y_train, y_temp = train_test_split(x, y, test_size=0.4, random_state=42)\n",
        "\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91cb56db",
      "metadata": {
        "id": "91cb56db"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=10000, output_dim=64),\n",
        "    LSTM(64, return_sequences=False),\n",
        "    Dropout(0.5),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6060276",
      "metadata": {
        "id": "a6060276"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.build(input_shape=(None,20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "058e6fd9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "058e6fd9",
        "outputId": "22a672fe-5a72-4094-b5c5-f053b766fa92"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "885e8d99",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "885e8d99",
        "outputId": "b97a1237-5035-4318-a851-171b1e837b0a"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=12,\n",
        "    batch_size=32,\n",
        "    validation_data=(x_val, y_val)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffb96554",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "ffb96554",
        "outputId": "94d24651-790e-45af-bdd3-16dbc9a93829"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['accuracy'], label='train acc')\n",
        "plt.plot(history.history['val_accuracy'], label='val acc')\n",
        "plt.legend()\n",
        "plt.title(\"Model Accuracy\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f557021",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f557021",
        "outputId": "c35bfe44-d8d4-413f-f395-b243ddbe9e3a"
      },
      "outputs": [],
      "source": [
        "preds = model.predict(x_test)\n",
        "preds_binary = (preds > 0.5).astype(int).flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e71cb88f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e71cb88f",
        "outputId": "5e91bf90-3a13-42d8-b20f-c3ec7b84bc61"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, preds_binary))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, preds_binary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "030e30be",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "030e30be",
        "outputId": "ba565e8d-e72f-4752-95a5-a764d20b57f2"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "print(f\"\\nAccuracy: {accuracy_score(y_test, preds_binary):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, preds_binary):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, preds_binary):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, preds_binary):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0032cc8b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "0032cc8b",
        "outputId": "3e0dcfd4-c3df-4433-96c1-f31dea9593cc"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test, preds_binary)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Negative', 'Positive'],\n",
        "            yticklabels=['Negative', 'Positive'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c38be90",
      "metadata": {
        "id": "9c38be90"
      },
      "source": [
        "---\n",
        "\n",
        "**Initial training results:** the model can be improved. Will try again.  \n",
        "**Second attempt:** model improved slightly. Will try again.  \n",
        "**Third attempt:** tried adding more epochs and batches. Will try again.  \n",
        "**Fourth attempt:** used lemmatization, better results. Will try again.\n",
        "**Fifth attempt:** improved stopword removal, model now performing decently. Will try again.  \n",
        "**Sixth attempt:** hyperparameter tuning using manual randomsearch and cross validation with stratified kfold. Will try to tune again.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
